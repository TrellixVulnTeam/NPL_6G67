{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--log_interval'], dest='log_interval', nargs=None, const=None, default=1000, type=<class 'int'>, choices=None, help='How often to show loss statistics and save samples.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
    "parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "parser.add_argument('--generate', action='store_true', help='Generate samples from a model.')\n",
    "parser.add_argument('--data_dir', default='./data/', help='Location of datasets.')\n",
    "parser.add_argument('--output_dir', default='./results/maf_mnist')\n",
    "parser.add_argument('--results_file', default='results.txt', help='Filename where to store settings and test results.')\n",
    "parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
    "# data\n",
    "parser.add_argument('--dataset', default='toy', help='Which dataset to use.')\n",
    "parser.add_argument('--flip_toy_var_order', action='store_true', help='Whether to flip the toy dataset variable order to (x2, x1).')\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed to use.')\n",
    "# model\n",
    "parser.add_argument('--model', default='maf', help='Which model to use: made, maf.')\n",
    "# made parameters\n",
    "parser.add_argument('--n_blocks', type=int, default=5, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
    "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
    "parser.add_argument('--hidden_size', type=int, default=100, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
    "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
    "parser.add_argument('--activation_fn', type=str, default='relu', help='What activation function to use in the MADEs.')\n",
    "parser.add_argument('--input_order', type=str, default='sequential', help='What input order to use (sequential | random).')\n",
    "parser.add_argument('--conditional', default=False, action='store_true', help='Whether to use a conditional model.')\n",
    "parser.add_argument('--no_batch_norm', action='store_true')\n",
    "# training params\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--n_epochs', type=int, default=50)\n",
    "parser.add_argument('--start_epoch', default=0, help='Starting epoch (for logging; to be overwritten when restoring file.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate.')\n",
    "parser.add_argument('--log_interval', type=int, default=1000, help='How often to show loss statistics and save samples.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--train --model=maf --dataset=MNIST --n_epochs=50\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cpu')\n",
    "torch.manual_seed(args.seed)\n",
    "if args.conditional: assert args.dataset in ['MNIST', 'CIFAR10'], 'Conditional inputs only available for labeled datasets MNIST and CIFAR10.'\n",
    "train_dataloader, test_dataloader = fetch_dataloaders(args.dataset, args.batch_size, args.device, args.flip_toy_var_order)\n",
    "args.input_size = train_dataloader.dataset.input_size\n",
    "args.input_dims = train_dataloader.dataset.input_dims\n",
    "args.cond_label_size = train_dataloader.dataset.label_size if args.conditional else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAF(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                    args.activation_fn, args.input_order, batch_norm=not args.no_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'MNIST',\n",
      " 'device': device(type='cpu'),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': (1, 28, 28),\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 784,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf_mnist',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/maf_mnist/results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'train': True}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=784, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=1568, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BatchNorm()\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=784, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=1568, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BatchNorm()\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=784, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=1568, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BatchNorm()\n",
      "    (6): MADE(\n",
      "      (net_input): MaskedLinear(in_features=784, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=1568, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BatchNorm()\n",
      "    (8): MADE(\n",
      "      (net_input): MaskedLinear(in_features=784, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=1568, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): BatchNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args.results_file = os.path.join(args.output_dir, args.results_file)\n",
    "\n",
    "print('Loaded settings and model:')\n",
    "print(pprint.pformat(args.__dict__))\n",
    "print(model)\n",
    "print(pprint.pformat(args.__dict__), file=open(args.results_file, 'a'))\n",
    "print(model, file=open(args.results_file, 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 / 50, step    0 / 600; loss 1689.1567\n",
      "Evaluate (epoch 0) -- logp(x) = -1662.797 +/- 2.399\n",
      "epoch   1 / 50, step    0 / 600; loss 1662.8104\n",
      "Evaluate (epoch 1) -- logp(x) = -1647.280 +/- 2.587\n",
      "epoch   2 / 50, step    0 / 600; loss 1610.2786\n",
      "Evaluate (epoch 2) -- logp(x) = -1633.524 +/- 2.519\n",
      "epoch   3 / 50, step    0 / 600; loss 1622.4014\n",
      "Evaluate (epoch 3) -- logp(x) = -1625.806 +/- 2.490\n",
      "epoch   4 / 50, step    0 / 600; loss 1603.7141\n",
      "Evaluate (epoch 4) -- logp(x) = -1620.161 +/- 2.479\n",
      "epoch   5 / 50, step    0 / 600; loss 1604.9921\n",
      "Evaluate (epoch 5) -- logp(x) = -1615.881 +/- 2.658\n",
      "epoch   6 / 50, step    0 / 600; loss 1627.3715\n",
      "Evaluate (epoch 6) -- logp(x) = -1612.920 +/- 2.551\n",
      "epoch   7 / 50, step    0 / 600; loss 1586.4985\n",
      "Evaluate (epoch 7) -- logp(x) = -1610.415 +/- 2.596\n",
      "epoch   8 / 50, step    0 / 600; loss 1598.1616\n",
      "Evaluate (epoch 8) -- logp(x) = -1608.782 +/- 2.634\n",
      "epoch   9 / 50, step    0 / 600; loss 1573.5225\n",
      "Evaluate (epoch 9) -- logp(x) = -1607.058 +/- 2.655\n",
      "epoch  10 / 50, step    0 / 600; loss 1597.5721\n",
      "Evaluate (epoch 10) -- logp(x) = -1605.594 +/- 2.608\n",
      "epoch  11 / 50, step    0 / 600; loss 1566.3549\n",
      "Evaluate (epoch 11) -- logp(x) = -1605.035 +/- 2.613\n",
      "epoch  12 / 50, step    0 / 600; loss 1591.2091\n",
      "Evaluate (epoch 12) -- logp(x) = -1603.547 +/- 2.605\n",
      "epoch  13 / 50, step    0 / 600; loss 1587.3701\n",
      "Evaluate (epoch 13) -- logp(x) = -1603.186 +/- 2.607\n",
      "epoch  14 / 50, step    0 / 600; loss 1599.3784\n",
      "Evaluate (epoch 14) -- logp(x) = -1601.845 +/- 2.647\n",
      "epoch  15 / 50, step    0 / 600; loss 1572.4253\n",
      "Evaluate (epoch 15) -- logp(x) = -1601.694 +/- 2.606\n",
      "epoch  16 / 50, step    0 / 600; loss 1589.0453\n",
      "Evaluate (epoch 16) -- logp(x) = -1601.193 +/- 2.639\n",
      "epoch  17 / 50, step    0 / 600; loss 1582.7145\n",
      "Evaluate (epoch 17) -- logp(x) = -1600.435 +/- 2.671\n",
      "epoch  18 / 50, step    0 / 600; loss 1574.7261\n",
      "Evaluate (epoch 18) -- logp(x) = -1600.033 +/- 2.675\n",
      "epoch  19 / 50, step    0 / 600; loss 1589.1672\n",
      "Evaluate (epoch 19) -- logp(x) = -1599.652 +/- 2.699\n",
      "epoch  20 / 50, step    0 / 600; loss 1579.8431\n",
      "Evaluate (epoch 20) -- logp(x) = -1599.320 +/- 2.707\n",
      "epoch  21 / 50, step    0 / 600; loss 1606.1403\n",
      "Evaluate (epoch 21) -- logp(x) = -1599.336 +/- 2.621\n",
      "epoch  22 / 50, step    0 / 600; loss 1566.4325\n",
      "Evaluate (epoch 22) -- logp(x) = -1598.794 +/- 2.759\n",
      "epoch  23 / 50, step    0 / 600; loss 1571.7965\n",
      "Evaluate (epoch 23) -- logp(x) = -1598.752 +/- 2.639\n",
      "epoch  24 / 50, step    0 / 600; loss 1557.4264\n",
      "Evaluate (epoch 24) -- logp(x) = -1598.375 +/- 2.669\n",
      "epoch  25 / 50, step    0 / 600; loss 1577.6881\n",
      "Evaluate (epoch 25) -- logp(x) = -1598.213 +/- 2.722\n",
      "epoch  26 / 50, step    0 / 600; loss 1570.6147\n",
      "Evaluate (epoch 26) -- logp(x) = -1598.399 +/- 2.676\n",
      "epoch  27 / 50, step    0 / 600; loss 1586.4932\n",
      "Evaluate (epoch 27) -- logp(x) = -1597.579 +/- 2.746\n",
      "epoch  28 / 50, step    0 / 600; loss 1583.2472\n",
      "Evaluate (epoch 28) -- logp(x) = -1598.328 +/- 2.732\n",
      "epoch  29 / 50, step    0 / 600; loss 1563.8790\n",
      "Evaluate (epoch 29) -- logp(x) = -1597.756 +/- 2.711\n",
      "epoch  30 / 50, step    0 / 600; loss 1585.4027\n",
      "Evaluate (epoch 30) -- logp(x) = -1597.964 +/- 2.711\n",
      "epoch  31 / 50, step    0 / 600; loss 1587.1422\n",
      "Evaluate (epoch 31) -- logp(x) = -1597.448 +/- 2.729\n",
      "epoch  32 / 50, step    0 / 600; loss 1570.8074\n",
      "Evaluate (epoch 32) -- logp(x) = -1597.808 +/- 2.793\n",
      "epoch  33 / 50, step    0 / 600; loss 1577.0563\n",
      "Evaluate (epoch 33) -- logp(x) = -1597.857 +/- 2.745\n",
      "epoch  34 / 50, step    0 / 600; loss 1568.6854\n",
      "Evaluate (epoch 34) -- logp(x) = -1597.647 +/- 2.716\n",
      "epoch  35 / 50, step    0 / 600; loss 1586.0869\n",
      "Evaluate (epoch 35) -- logp(x) = -1597.491 +/- 2.815\n",
      "epoch  36 / 50, step    0 / 600; loss 1561.7251\n",
      "Evaluate (epoch 36) -- logp(x) = -1597.923 +/- 2.802\n",
      "epoch  37 / 50, step    0 / 600; loss 1571.4164\n",
      "Evaluate (epoch 37) -- logp(x) = -1597.744 +/- 2.754\n",
      "epoch  38 / 50, step    0 / 600; loss 1549.7935\n",
      "Evaluate (epoch 38) -- logp(x) = -1597.827 +/- 2.834\n",
      "epoch  39 / 50, step    0 / 600; loss 1575.1182\n",
      "Evaluate (epoch 39) -- logp(x) = -1597.675 +/- 2.754\n",
      "epoch  40 / 50, step    0 / 600; loss 1557.7394\n",
      "Evaluate (epoch 40) -- logp(x) = -1597.987 +/- 2.804\n",
      "epoch  41 / 50, step    0 / 600; loss 1562.3914\n",
      "Evaluate (epoch 41) -- logp(x) = -1597.966 +/- 2.923\n",
      "epoch  42 / 50, step    0 / 600; loss 1554.9717\n",
      "Evaluate (epoch 42) -- logp(x) = -1598.556 +/- 2.958\n",
      "epoch  43 / 50, step    0 / 600; loss 1581.8108\n",
      "Evaluate (epoch 43) -- logp(x) = -1597.822 +/- 2.823\n",
      "epoch  44 / 50, step    0 / 600; loss 1559.7957\n",
      "Evaluate (epoch 44) -- logp(x) = -1598.801 +/- 2.861\n",
      "epoch  45 / 50, step    0 / 600; loss 1553.6786\n",
      "Evaluate (epoch 45) -- logp(x) = -1598.208 +/- 2.905\n",
      "epoch  46 / 50, step    0 / 600; loss 1582.3873\n",
      "Evaluate (epoch 46) -- logp(x) = -1598.353 +/- 2.897\n",
      "epoch  47 / 50, step    0 / 600; loss 1610.2837\n",
      "Evaluate (epoch 47) -- logp(x) = -1599.098 +/- 2.995\n",
      "epoch  48 / 50, step    0 / 600; loss 1572.4313\n",
      "Evaluate (epoch 48) -- logp(x) = -1598.508 +/- 2.859\n",
      "epoch  49 / 50, step    0 / 600; loss 1558.7681\n",
      "Evaluate (epoch 49) -- logp(x) = -1598.531 +/- 2.928\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, train_dataloader.dataset.lam, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model.eval()\n",
    "all_Z = np.zeros((0, 784))\n",
    "all_J = np.zeros((0, 784))\n",
    "import pdb\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    # check if labeled dataset\n",
    "    if len(data) == 1:\n",
    "        x, y = data[0], None\n",
    "    else:\n",
    "        x, y = data\n",
    "        y = y.to(args.device)\n",
    "    x = x.view(x.shape[0], -1).to(args.device)\n",
    "    u, sum_log_abs_det_jacobians = model(x)\n",
    "    u_n = u.detach().numpy()\n",
    "    all_Z = np.vstack((all_Z, u_n))\n",
    "    j_n = sum_log_abs_det_jacobians.detach().numpy()\n",
    "    all_J = np.vstack((all_J, j_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Z.csv\", all_Z, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X=pca.fit_transform(all_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-e93cb78307a9>:2: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.88765944e-02, -5.51762767e-02,  1.12748560e-02, -6.07067198e-02,\n",
       "        3.23312868e-03,  2.22165896e-02, -9.21576362e-03, -1.26263284e-03,\n",
       "        6.64300100e-03, -1.09491504e-02,  5.87760935e-02, -2.63461892e-02,\n",
       "       -2.52269711e-04,  3.63298760e-02,  2.81959522e-02, -1.36317622e-02,\n",
       "       -6.44017911e-03,  3.84784945e-02,  2.46643076e-02, -1.91733178e-02,\n",
       "       -1.62476609e-02, -4.40315149e-02, -2.45038210e-02,  3.92330037e-02,\n",
       "        3.61201974e-02, -2.28748634e-02, -3.17254403e-02,  1.65577713e-02,\n",
       "       -2.62301030e-02,  6.57028353e-05,  3.15733209e-02,  1.69996257e-02,\n",
       "        8.14984135e-03,  3.83423505e-02, -7.45518909e-02,  2.40184378e-03,\n",
       "       -1.48085615e-02,  7.04247820e-03,  6.40777527e-03,  9.51270697e-03,\n",
       "       -2.77288046e-02, -1.08414332e-03,  9.06339071e-03, -1.47761939e-02,\n",
       "        1.14787243e-02,  2.43947661e-02,  2.31564527e-02, -9.03986044e-03,\n",
       "       -3.64575656e-02, -1.79194306e-02,  4.06408540e-02,  1.61644810e-02,\n",
       "        1.51516607e-02, -9.37554793e-03,  1.46234538e-04, -1.37966895e-02,\n",
       "        4.10486497e-02,  2.05172997e-02, -2.95261096e-02, -2.21476330e-02,\n",
       "        1.87624744e-02, -3.68597074e-03,  2.31067563e-02,  2.17649857e-02,\n",
       "       -9.15672722e-03, -1.54024335e-02, -5.30799226e-03,  6.70006633e-03,\n",
       "       -2.34954666e-03, -1.75827465e-02, -1.87970994e-02, -2.75704901e-02,\n",
       "       -2.69849353e-02,  2.12637884e-02,  8.91586075e-03, -1.49867275e-02,\n",
       "        3.51775344e-02,  8.06343274e-03,  9.67276882e-03, -2.13371348e-02,\n",
       "        2.12954998e-04, -2.46150785e-02,  1.09653026e-02,  1.23399402e-03,\n",
       "        1.13086295e-02, -1.51783465e-02, -6.92246328e-03, -2.55633181e-02,\n",
       "        2.20120534e-02,  3.02025784e-02, -3.31625260e-02, -1.11167768e-02,\n",
       "       -1.13987621e-02, -1.11917089e-02, -1.12204064e-02, -3.17892253e-02,\n",
       "        6.83304127e-03,  1.79872581e-02,  3.24574796e-02, -2.77213129e-02,\n",
       "        3.28500570e-02, -1.72446664e-02, -2.23541427e-02,  3.06015053e-02,\n",
       "        3.33244072e-02,  1.29898504e-03,  6.70264565e-03,  3.73268618e-02,\n",
       "       -7.70022405e-03, -3.76061094e-02,  7.36619722e-03, -5.55783592e-02,\n",
       "        1.10584906e-03, -8.79861080e-04,  1.35544239e-02,  2.14833417e-02,\n",
       "        4.40081424e-03, -1.05210497e-02,  1.21651243e-02,  1.39289731e-02,\n",
       "       -1.62721352e-02,  1.18026450e-03,  1.50542937e-02, -3.12335603e-02,\n",
       "       -6.97316512e-04, -1.00041710e-02, -2.60555136e-02, -1.81982706e-02,\n",
       "       -8.19663889e-03, -4.71337935e-03, -3.29767654e-02, -5.61045547e-02,\n",
       "       -4.44571233e-02, -2.32781332e-02,  9.23560957e-03,  4.90383849e-02,\n",
       "       -3.47610052e-03,  2.23202689e-02,  3.94478137e-02, -1.23781561e-02,\n",
       "        3.15353357e-03, -4.49121524e-03, -4.08986196e-03, -4.40063139e-02,\n",
       "        4.51466925e-02,  3.99744017e-02,  1.41750242e-02,  3.35016727e-02,\n",
       "        2.36228113e-02, -1.21395005e-02,  1.76095887e-03, -1.07700712e-02,\n",
       "        2.13220377e-02,  8.16048460e-03,  1.07625196e-02, -2.23630913e-02,\n",
       "        8.02986773e-03, -6.47717656e-03, -8.48685449e-03, -2.24347096e-02,\n",
       "        1.22014298e-02,  9.25818810e-05, -2.11219576e-02,  6.03728671e-03,\n",
       "       -6.66182316e-03,  1.64928038e-04,  1.25151198e-02,  1.78594376e-02,\n",
       "       -3.31153367e-02,  1.67768778e-03, -2.25328077e-03, -2.32683066e-02,\n",
       "        1.46733736e-02, -5.35482610e-03, -2.95558862e-03,  8.92319619e-03,\n",
       "        2.73316597e-02, -1.79944700e-02,  5.26215333e-03,  1.53838658e-02,\n",
       "        1.45459258e-02, -1.06277403e-02,  2.06557599e-03,  4.75572398e-05,\n",
       "        1.86360225e-02, -3.24558056e-02, -4.55995901e-03, -7.70021987e-03,\n",
       "        6.68739679e-03, -9.38358172e-03, -1.50293160e-02,  1.26897141e-02,\n",
       "        2.94578519e-02,  1.59227334e-02,  1.95068449e-03,  1.29113972e-02,\n",
       "       -8.79140411e-03,  8.78187379e-03, -4.28249246e-02,  1.12109875e-02,\n",
       "        3.83142254e-02, -2.05684095e-02,  2.01171332e-03,  2.32010403e-03,\n",
       "       -1.66582708e-04,  2.16629425e-02,  2.22640482e-02, -1.31189409e-02,\n",
       "        1.23595298e-02,  1.16384920e-02,  2.71981176e-02,  7.22372042e-03,\n",
       "        1.03550699e-02, -3.26059695e-02,  4.32671996e-03, -1.08931337e-02,\n",
       "       -3.17617510e-02, -3.41371519e-02,  1.53306374e-02,  2.37853942e-02,\n",
       "        5.67631538e-05,  1.48947608e-02,  5.85456053e-02, -6.19477667e-02,\n",
       "       -8.37370241e-03,  2.16980080e-02,  1.90023525e-02,  1.83525032e-02,\n",
       "        3.24957995e-02, -1.13567625e-02, -1.66408577e-02,  9.89989171e-03,\n",
       "        1.14613209e-02, -5.71484669e-03, -3.98086903e-02, -1.93336789e-02,\n",
       "        9.43267275e-04,  8.05764327e-04,  4.04880572e-03, -1.44741856e-02,\n",
       "       -2.46484898e-02, -4.71688154e-02, -3.39141142e-02, -4.30275349e-02,\n",
       "       -2.71001902e-02, -3.81902868e-02, -2.53553874e-02,  8.52725200e-03,\n",
       "        1.29827546e-02, -2.16958547e-02, -5.02938680e-03,  2.97152639e-03,\n",
       "       -3.78652686e-02,  1.42152209e-04,  5.27149802e-03,  3.70099006e-02,\n",
       "       -3.36957799e-02, -4.11687305e-02,  2.36593688e-03,  3.42643937e-03,\n",
       "       -2.95966995e-03, -6.41219123e-02, -5.85245042e-02, -2.78683658e-02,\n",
       "       -1.15462500e-02,  7.99740205e-03, -5.79354869e-03, -4.03075808e-02,\n",
       "       -5.93581161e-02, -4.97355474e-02, -4.63072228e-02, -2.84192667e-02,\n",
       "       -6.78999642e-02, -3.72665786e-02, -3.07825455e-02,  5.27234527e-02,\n",
       "       -6.11732978e-03,  5.25913319e-02, -2.15393511e-02,  1.07956927e-02,\n",
       "        3.34229183e-02, -9.11600679e-03,  2.68516544e-03, -4.00818247e-02,\n",
       "       -3.52540492e-02, -6.60971231e-02,  1.38054903e-02, -2.08038307e-02,\n",
       "       -3.68092238e-02, -4.49652718e-02, -4.95205923e-02, -2.51388699e-02,\n",
       "       -1.07303140e-02,  1.85020976e-02, -8.81586609e-03, -3.05878360e-02,\n",
       "       -5.02886335e-02, -3.35144548e-02, -3.48120010e-02, -3.22212188e-02,\n",
       "       -5.31661673e-02, -3.11603087e-02, -1.65948112e-02,  4.54909628e-02,\n",
       "        1.60526583e-02,  2.92443834e-02,  3.73829939e-04, -2.99212150e-02,\n",
       "       -6.26237817e-03, -1.36133416e-02, -8.39033634e-03, -2.74227229e-02,\n",
       "       -5.83195955e-02, -4.02903851e-02, -2.04411055e-02, -2.47132394e-02,\n",
       "       -3.36826804e-02, -3.94853417e-02, -2.39137311e-02, -6.64966129e-03,\n",
       "        2.34557261e-03, -2.36071787e-03, -2.80726196e-02, -3.52614251e-02,\n",
       "       -2.42402100e-02, -9.25901557e-03, -2.28801474e-02, -4.15733194e-02,\n",
       "       -5.31962172e-02, -4.53280212e-02, -2.19489645e-02, -1.17672169e-02,\n",
       "       -1.51096147e-02, -3.95609581e-03, -1.48051877e-02,  4.21137356e-02,\n",
       "        3.45129197e-02,  1.82574718e-02, -1.75591611e-02, -2.23503770e-02,\n",
       "       -4.78643784e-02, -5.78668719e-02, -4.43840165e-02, -5.03778749e-02,\n",
       "       -2.70409301e-02, -3.44631476e-02, -1.77106346e-02, -1.19369007e-02,\n",
       "       -1.41785584e-02, -3.23918236e-02, -2.71826476e-02, -2.27610012e-02,\n",
       "        1.27385005e-03,  3.49027048e-03, -1.18431475e-02, -3.76846253e-02,\n",
       "       -4.35772340e-02, -3.46338811e-02, -4.08421930e-02, -1.69288833e-02,\n",
       "       -1.45810729e-02,  1.40737280e-02,  6.66690901e-03, -1.95074173e-02,\n",
       "       -9.74707310e-03, -1.80620759e-02,  3.64314594e-02, -1.29399038e-02,\n",
       "       -5.83590088e-02, -5.89853605e-02, -4.35364584e-02, -6.42062777e-02,\n",
       "       -3.81612613e-02, -2.32101079e-02, -6.18798226e-04, -5.24138629e-03,\n",
       "       -2.23171274e-02, -2.82407071e-02,  1.69009309e-03,  1.65535932e-02,\n",
       "        3.19024875e-02,  2.12114328e-02,  8.59674715e-03, -1.39825716e-02,\n",
       "       -3.55708999e-02, -3.93353245e-02, -2.06481600e-02, -1.57575813e-02,\n",
       "       -4.74003674e-02, -1.14286294e-02, -1.96040677e-02, -6.27403758e-03,\n",
       "        9.53700847e-03, -7.97454867e-03, -5.85862969e-03, -3.95804790e-03,\n",
       "       -1.73031219e-02, -6.67257507e-02, -5.55568252e-02, -4.36589921e-02,\n",
       "       -1.58825679e-02, -1.52201149e-02,  1.52828149e-02,  5.79563469e-03,\n",
       "       -2.35296642e-02,  8.23748882e-03,  3.02976761e-02,  1.30587793e-02,\n",
       "        1.72107965e-02,  2.63834502e-02,  1.21291302e-02, -1.05128157e-02,\n",
       "       -1.45473778e-02, -2.68042083e-02, -3.25843646e-02, -1.58242520e-02,\n",
       "       -2.03563126e-02, -4.92971766e-03, -2.15448038e-02, -1.54212720e-02,\n",
       "        3.86049927e-03, -1.33585867e-02, -1.62324637e-02, -1.54259499e-02,\n",
       "        1.28944738e-02, -3.16212897e-02, -3.06454126e-02, -2.51693197e-02,\n",
       "       -2.12132662e-02,  1.29704088e-02,  8.41849508e-03,  1.49330395e-02,\n",
       "        1.18301599e-02,  6.15104211e-02,  5.05489529e-02,  1.68520738e-02,\n",
       "       -5.66148043e-03, -7.41075368e-03, -8.27120868e-03, -1.29682512e-02,\n",
       "       -5.89369098e-03, -4.81604615e-03, -8.82304941e-03, -3.98061950e-02,\n",
       "       -2.63113661e-02,  1.33402007e-03,  7.53648947e-04, -9.86351245e-03,\n",
       "       -3.41565213e-02,  2.86850747e-02,  1.71583351e-02,  3.30934219e-02,\n",
       "        2.32576679e-02,  3.75488806e-03, -1.64741195e-02, -1.58389484e-02,\n",
       "       -1.57778605e-02,  6.59531035e-03,  1.09689237e-02,  1.44879774e-02,\n",
       "        4.21963621e-02,  6.08721501e-02,  4.90737512e-02,  6.64852653e-03,\n",
       "       -1.87251097e-02, -3.66581655e-02, -2.28090734e-02, -1.88501559e-02,\n",
       "       -5.57980910e-03, -1.57507500e-02,  5.67744600e-03, -2.12079182e-02,\n",
       "       -1.81338700e-02, -6.98616960e-02, -2.98339336e-02,  4.00343072e-02,\n",
       "       -6.09520805e-02,  3.39998929e-03,  1.49722851e-02,  9.64754123e-03,\n",
       "        3.94129357e-02, -2.41760748e-02, -1.89204457e-02, -4.63670182e-03,\n",
       "        1.08591867e-02,  1.53620299e-02,  2.73853633e-02,  4.98783934e-02,\n",
       "        3.33868902e-02,  5.66689352e-02,  2.63362401e-02, -2.08174828e-03,\n",
       "       -4.01654306e-02, -4.29807461e-02, -3.63204321e-02, -2.86224309e-02,\n",
       "       -2.41219845e-02, -3.98107574e-02, -2.19513296e-02, -2.39638999e-02,\n",
       "       -6.79371203e-04, -1.47759471e-03,  1.36328908e-03, -8.62691639e-03,\n",
       "        7.38753754e-02, -3.41195508e-02, -1.43934327e-02,  2.08725018e-02,\n",
       "        8.19966195e-03, -4.27101617e-02, -3.15273272e-02, -5.60958496e-02,\n",
       "       -1.32953638e-02, -2.22737544e-03,  3.85141754e-02,  2.95416575e-02,\n",
       "        4.14269524e-02,  3.26643743e-02,  2.97035209e-03, -1.71100723e-02,\n",
       "       -2.82193185e-02, -4.28523834e-02, -2.78989608e-02, -1.73762627e-02,\n",
       "       -2.73675216e-02, -1.74403609e-02, -1.49615948e-02, -2.40257276e-02,\n",
       "       -3.17914480e-03, -1.52944181e-02,  3.90919023e-02, -1.26824857e-02,\n",
       "       -2.41442716e-02,  3.49150307e-02, -2.59502714e-03,  1.74413681e-02,\n",
       "        2.78084274e-02, -6.33149883e-02, -4.47580223e-02, -4.10594295e-02,\n",
       "       -5.35878757e-02, -9.88321497e-03,  2.95041535e-02,  1.57820414e-02,\n",
       "        3.22979310e-02,  3.11419933e-02,  2.74295120e-02,  1.70845228e-02,\n",
       "        3.27437009e-03, -3.15615362e-02, -1.42729155e-03, -6.23480868e-03,\n",
       "       -1.99012885e-02, -2.83687574e-02, -1.71308883e-02, -1.71590686e-02,\n",
       "       -1.76474939e-02, -1.16705381e-03, -2.12476971e-02,  4.36428603e-02,\n",
       "        1.38184821e-02, -2.40202216e-02, -4.02196911e-03,  2.02468264e-02,\n",
       "       -6.69511475e-03, -4.61083898e-02, -4.06459124e-02, -3.84943304e-02,\n",
       "       -5.11975957e-02, -1.12454194e-02,  9.84733577e-03,  2.28331360e-02,\n",
       "        9.40151575e-03,  3.72927077e-02,  4.14026746e-02,  1.81462622e-02,\n",
       "        1.51856435e-02, -9.64313626e-03, -1.90275273e-02, -2.11415244e-03,\n",
       "        1.90265172e-02,  6.01486163e-03, -1.38863409e-02, -3.75569300e-02,\n",
       "       -4.32642888e-02,  4.08859834e-02, -1.66790937e-02,  2.27877315e-02,\n",
       "       -4.46250781e-02,  2.14356038e-02, -2.02705300e-02, -3.58249950e-03,\n",
       "       -2.61476862e-02, -1.98872712e-02, -2.78520114e-02, -9.46930492e-03,\n",
       "       -3.46868031e-02,  5.79766010e-03, -1.15107816e-02, -8.25926178e-03,\n",
       "       -2.09984382e-02,  1.09555278e-02,  3.31398583e-02,  2.08457921e-02,\n",
       "        1.09864211e-02, -6.73235306e-03, -4.45211325e-03,  9.03204305e-04,\n",
       "        3.64797592e-02,  2.60799674e-03,  3.17883276e-03, -1.84591878e-02,\n",
       "       -3.55562432e-02, -1.63502213e-02, -3.75876273e-02, -2.86681089e-02,\n",
       "        2.39716949e-02,  1.48324326e-02, -4.76549890e-02, -6.67903420e-03,\n",
       "        1.52443687e-02, -1.10994202e-02, -3.54770076e-02, -4.78626800e-02,\n",
       "       -4.73271872e-02, -2.36618207e-03, -5.42183720e-02, -1.30037682e-02,\n",
       "       -7.50838664e-03,  4.67871382e-03,  9.84063480e-03, -1.12957283e-02,\n",
       "       -1.66696030e-03, -2.10285816e-02,  4.84986395e-03, -2.09255048e-02,\n",
       "        3.29930111e-02,  2.44405473e-02,  7.42641183e-03,  1.31826167e-02,\n",
       "       -1.59448373e-02,  3.63430663e-02,  3.49399617e-03,  2.13066157e-02,\n",
       "        3.52053413e-02, -1.07134260e-02,  2.69294943e-02,  5.19158578e-03,\n",
       "        1.43483226e-02,  1.89228820e-02, -2.15353839e-02, -2.81924975e-02,\n",
       "       -1.76884834e-02, -2.98870030e-02, -3.28129793e-02, -1.69572039e-02,\n",
       "        2.80955296e-02, -1.00424382e-02, -1.07797409e-02, -1.47581763e-02,\n",
       "        1.03582381e-03, -3.18384427e-02,  2.88174932e-02,  1.02423048e-02,\n",
       "        1.52803306e-02,  2.53317081e-02, -1.94370004e-02, -3.69388477e-02,\n",
       "        1.50747735e-02,  8.41388066e-03,  7.10462946e-03,  2.06122657e-02,\n",
       "        5.91871191e-03, -1.15604315e-02, -2.95415507e-03,  2.57072611e-02,\n",
       "        1.35165122e-02,  2.22432176e-02, -3.21882259e-02,  2.78562042e-02,\n",
       "        1.42236356e-02, -1.14741900e-02, -1.21115924e-02, -1.32561099e-02,\n",
       "       -1.03617139e-02,  1.00410728e-01,  2.76857710e-02,  6.30629088e-02,\n",
       "       -2.68824102e-03, -2.91786793e-02,  1.51305657e-02,  6.66055927e-03,\n",
       "       -5.84173020e-02,  3.58295423e-02, -9.81916415e-03,  1.66123012e-02,\n",
       "        2.86933207e-02, -1.63476351e-02, -2.76223825e-02, -2.48635414e-03,\n",
       "       -5.60438744e-02, -7.44280248e-03,  3.76627302e-02, -3.17884996e-03,\n",
       "        3.37460769e-02,  3.21577632e-02, -4.38079191e-02, -1.70491478e-02,\n",
       "        1.99637727e-02,  3.38464374e-02, -2.15175032e-02, -4.00288658e-02,\n",
       "        1.89012453e-03,  2.11857234e-04,  5.28717164e-02,  2.44045469e-02,\n",
       "        1.95095309e-02, -1.79223848e-02, -2.38760667e-02, -5.63985946e-02,\n",
       "        3.10181812e-03, -2.22796184e-02, -2.45029658e-02, -1.64054846e-02,\n",
       "        5.76596291e-03,  2.17441564e-02,  4.83851949e-02, -2.06198398e-02,\n",
       "        3.13056825e-02, -7.43776336e-03, -1.03397990e-02, -5.94225600e-02,\n",
       "       -2.58443719e-02, -2.88978016e-03,  1.06560826e-02,  4.60192887e-02,\n",
       "        1.05738282e-03, -4.85123508e-02,  5.42891329e-02,  5.18648977e-02,\n",
       "       -9.94036176e-03,  4.98295351e-02, -1.98172075e-02, -2.01669610e-02,\n",
       "       -4.91134347e-03,  1.69445479e-03,  2.11061240e-02,  1.25886540e-02,\n",
       "        1.97976865e-02, -1.17304234e-02,  5.70126847e-02, -8.07235618e-03,\n",
       "       -2.67986007e-02, -7.31608375e-02,  6.54012290e-03,  3.05050708e-02,\n",
       "       -3.09066280e-03, -8.87625581e-03, -3.37549554e-02,  2.19705874e-02,\n",
       "       -3.50692521e-02, -1.30242797e-02, -1.49963429e-03, -3.87784952e-02,\n",
       "        9.52877507e-03, -3.68100433e-02, -3.72344402e-02, -2.94473301e-02,\n",
       "       -3.67546317e-03, -1.18794551e-02, -2.63367787e-02,  3.66979903e-03,\n",
       "        1.14731388e-02,  9.89565820e-03, -2.53934522e-02,  3.38231968e-03,\n",
       "       -1.18100457e-03,  6.16479312e-02, -5.16355933e-02, -8.41522191e-02,\n",
       "        2.61789509e-02, -2.70267170e-02, -5.33289334e-02,  2.35701222e-02])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_Z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04976354, 1.00542878, 1.00180423, 1.01737874, 1.00549958,\n",
       "       1.01039342, 1.01202564, 1.05135164, 0.99023927, 1.01122651,\n",
       "       0.97200982, 1.04300541, 1.00258576, 1.01150667, 1.01926005,\n",
       "       1.00373497, 1.02045127, 0.96275423, 0.95577446, 1.02327059,\n",
       "       1.01158343, 1.01839834, 1.00299485, 0.94772549, 1.01429682,\n",
       "       1.03504428, 1.01945449, 1.00892511, 1.03829897, 1.01120557,\n",
       "       0.973932  , 0.94712612, 1.00198936, 0.94327696, 1.06532138,\n",
       "       0.98187298, 1.02548556, 0.98881249, 1.00499885, 0.95672944,\n",
       "       0.96856386, 0.99974339, 0.99565594, 0.97428335, 0.98018999,\n",
       "       0.96985408, 0.9761144 , 0.9928966 , 1.01821399, 1.03637732,\n",
       "       0.98142833, 1.02742561, 0.97636353, 1.00743014, 0.98820771,\n",
       "       1.05074385, 0.98745265, 0.98092373, 1.0151592 , 1.03221518,\n",
       "       0.99625426, 1.00757363, 1.03211199, 1.00540985, 1.00389014,\n",
       "       0.96000655, 0.96114561, 0.96395515, 0.95503512, 0.94675772,\n",
       "       0.91731205, 0.92953513, 0.9238405 , 0.89820126, 0.91839452,\n",
       "       0.92627219, 0.88342508, 0.96327718, 0.97916325, 0.96533966,\n",
       "       0.98253484, 1.05408451, 1.01125049, 1.00421006, 1.01338332,\n",
       "       0.99511902, 0.99232338, 0.99736434, 1.03293282, 0.96780178,\n",
       "       1.07062594, 0.95977357, 0.94243615, 0.87904876, 0.93655974,\n",
       "       0.95124207, 0.94219454, 0.94759589, 1.02632358, 0.9989605 ,\n",
       "       0.97310527, 0.9414496 , 0.93447188, 0.88878407, 0.92383848,\n",
       "       1.01334643, 1.00774299, 0.99075854, 1.07508204, 1.00673414,\n",
       "       0.97673346, 1.07720308, 0.99744726, 1.02912354, 1.01310758,\n",
       "       1.01624763, 0.99434636, 1.06577378, 1.01061013, 0.94571164,\n",
       "       0.94524721, 0.94306687, 0.98339852, 0.98100352, 0.97036929,\n",
       "       0.99735656, 0.99281558, 0.97564863, 0.95811212, 0.93226353,\n",
       "       0.97492122, 0.96160592, 0.9837713 , 0.96749117, 0.97266204,\n",
       "       1.00817791, 1.06051593, 1.00475877, 0.96188002, 0.9905077 ,\n",
       "       1.02427179, 1.00103311, 0.98559829, 0.99795289, 1.00568195,\n",
       "       1.0067642 , 0.98013756, 0.93336134, 0.97041689, 0.99221322,\n",
       "       1.01996167, 1.00454887, 0.97726827, 1.00006033, 0.9738721 ,\n",
       "       0.99865979, 0.97895638, 0.99178214, 1.01097132, 0.9605125 ,\n",
       "       0.93618541, 0.97559843, 1.00034553, 0.98191312, 1.06973094,\n",
       "       1.0225371 , 1.00885832, 1.02477684, 1.06033154, 1.00422107,\n",
       "       1.00317596, 1.04283355, 1.03546586, 0.97638237, 0.94064215,\n",
       "       0.95003819, 0.94065739, 0.98311117, 0.96256181, 0.98575128,\n",
       "       0.98353911, 0.99633206, 0.98837161, 1.01193588, 0.99357648,\n",
       "       1.01293362, 0.96892174, 0.93353834, 0.93345667, 0.97058538,\n",
       "       0.94583621, 0.99943733, 1.01306151, 1.04914194, 1.00448989,\n",
       "       0.96547354, 0.96869039, 1.00792003, 1.04627228, 1.00395765,\n",
       "       1.03678717, 1.04608675, 0.96595588, 0.98505737, 1.02349386,\n",
       "       0.97420858, 0.97624102, 1.01498846, 1.00248152, 0.99591356,\n",
       "       0.98725385, 0.99201329, 0.98647644, 1.00542018, 1.00548179,\n",
       "       0.96390516, 0.97192383, 0.97047532, 0.97986266, 1.00809372,\n",
       "       0.99146791, 1.00484927, 0.9342005 , 1.02392909, 0.98480644,\n",
       "       0.94799231, 1.00174917, 0.98289176, 0.98471283, 0.97450572,\n",
       "       0.95483993, 0.99050797, 0.97728418, 0.97035656, 1.00310614,\n",
       "       0.98144754, 0.98595336, 1.00697657, 0.99701089, 1.01036307,\n",
       "       0.9994111 , 1.00815234, 0.9985137 , 0.96767115, 0.94985629,\n",
       "       0.93865806, 0.96820625, 1.02022204, 1.01657589, 1.03523234,\n",
       "       0.99559148, 1.01960196, 1.01735051, 1.00091741, 0.9859105 ,\n",
       "       0.92125415, 0.9413373 , 0.94565055, 0.95456994, 0.99322045,\n",
       "       0.97651198, 0.99728325, 0.99814273, 0.98810096, 1.00205422,\n",
       "       1.00756483, 0.99661712, 0.99347815, 0.98879576, 1.01466027,\n",
       "       1.0107    , 0.96036928, 0.96508574, 0.96831129, 0.96576101,\n",
       "       0.96735745, 0.99774683, 0.97473904, 1.00648479, 0.99196873,\n",
       "       0.97745584, 0.98422689, 0.95165403, 0.98015178, 0.93859052,\n",
       "       0.89056761, 0.92238179, 0.97179093, 0.97257039, 0.99226611,\n",
       "       1.00074772, 0.99373495, 1.01317521, 0.99587233, 1.00232189,\n",
       "       0.99373436, 0.98948444, 1.00764517, 1.001393  , 0.96112298,\n",
       "       0.9695685 , 0.97170046, 0.9455857 , 0.95687769, 0.94285542,\n",
       "       0.98869651, 1.00039867, 1.008344  , 0.97440857, 1.02908725,\n",
       "       0.92879127, 0.94177957, 0.90318487, 0.89176483, 0.92403711,\n",
       "       0.97794726, 0.99555466, 1.00575173, 1.00507377, 1.01466948,\n",
       "       0.99995577, 1.0128675 , 1.00456148, 0.98881777, 0.98581521,\n",
       "       1.00490962, 0.99320391, 0.97464347, 0.97078538, 0.96607226,\n",
       "       0.95542926, 0.96649379, 0.98587466, 1.00530846, 1.07335101,\n",
       "       1.0038078 , 0.98236654, 0.95513651, 0.97713948, 0.98223444,\n",
       "       0.8730141 , 0.87702611, 0.91274155, 0.95161195, 0.97432776,\n",
       "       1.0085322 , 1.00088986, 1.00761732, 1.00424973, 1.00081233,\n",
       "       0.98736797, 0.99336857, 0.99420985, 1.0171589 , 0.99833482,\n",
       "       0.97583457, 0.98313887, 0.96537601, 0.93542348, 0.92363235,\n",
       "       0.93884677, 0.95835311, 0.95240627, 0.99212565, 1.04337375,\n",
       "       1.03882058, 1.02806237, 0.94719532, 0.87208789, 0.83819409,\n",
       "       0.91444609, 0.95155331, 0.98332313, 1.01058053, 1.00438685,\n",
       "       1.00675484, 0.99883893, 0.9813439 , 0.99645555, 1.00507871,\n",
       "       1.00218354, 1.02261516, 0.99870905, 0.98763105, 0.98093897,\n",
       "       0.96087378, 0.92154782, 0.94788639, 0.93817406, 0.93354297,\n",
       "       1.00223598, 0.97530849, 0.99226166, 1.01967167, 1.05543941,\n",
       "       0.94178425, 0.89024246, 0.89022484, 0.93904875, 0.96704556,\n",
       "       0.98928853, 1.01618039, 1.02895179, 1.0163759 , 1.00377284,\n",
       "       0.99719555, 0.99862074, 0.99537754, 0.99684569, 1.00216638,\n",
       "       0.9913307 , 0.99939565, 0.98497144, 0.96796635, 0.97313062,\n",
       "       0.9321806 , 0.8810353 , 0.93548841, 1.00375422, 1.00734877,\n",
       "       0.9810439 , 1.01315663, 0.95201693, 0.9568442 , 0.94224711,\n",
       "       0.93228166, 0.97781587, 0.97232103, 0.99896988, 1.00143329,\n",
       "       1.00321713, 0.99111226, 1.00091444, 0.97873266, 0.99247414,\n",
       "       0.98343293, 0.9911093 , 1.00322068, 0.98147731, 0.98180445,\n",
       "       0.98189321, 0.96213996, 0.96630709, 0.95659975, 0.97507558,\n",
       "       0.93918038, 0.94786286, 0.97007215, 1.00458908, 0.97629245,\n",
       "       1.03104331, 1.02067732, 0.95610194, 0.95749461, 0.99073848,\n",
       "       1.00127067, 0.99622627, 1.00232794, 1.00811774, 0.9963913 ,\n",
       "       0.98660373, 0.98137461, 1.01003893, 1.00877447, 1.00289235,\n",
       "       1.00076574, 0.98826337, 0.98064665, 0.99097505, 1.02156056,\n",
       "       0.9554937 , 0.95811359, 0.97034805, 1.03748621, 1.03517725,\n",
       "       0.98623904, 1.05291177, 1.02831393, 0.97363313, 1.03024501,\n",
       "       0.95860887, 0.95840335, 1.00123152, 0.99755565, 1.0145584 ,\n",
       "       1.02226139, 1.02751436, 1.0169436 , 1.02399303, 1.00329006,\n",
       "       0.99932527, 0.99550788, 1.00219883, 1.00081886, 0.97611479,\n",
       "       0.97174299, 0.98252563, 0.97940733, 0.96420235, 0.95327649,\n",
       "       0.95251071, 1.01148668, 0.97388129, 1.02579757, 0.94216818,\n",
       "       1.00051146, 1.03979459, 1.07136067, 0.98313036, 0.95336663,\n",
       "       0.92458994, 0.99073161, 0.99167035, 1.01044439, 1.03231093,\n",
       "       1.00710065, 0.99709894, 0.99407236, 0.98539731, 0.98756552,\n",
       "       0.98062443, 1.00758008, 0.99020001, 0.96528836, 0.97075068,\n",
       "       0.95694793, 0.95313083, 0.95906449, 0.99413767, 1.02034311,\n",
       "       0.98555272, 0.99577113, 1.03032553, 0.97567507, 1.08202291,\n",
       "       0.97969125, 0.98238704, 0.94257839, 0.93338359, 0.92012691,\n",
       "       0.96364629, 0.97302571, 0.98747473, 0.98180544, 0.95921016,\n",
       "       0.97612304, 0.97827849, 0.97093832, 0.9697327 , 0.99778003,\n",
       "       0.98608895, 0.98434054, 0.97404441, 0.9486938 , 0.92466533,\n",
       "       0.92255881, 0.92657779, 0.96119852, 1.07724626, 0.94615863,\n",
       "       0.9770964 , 1.05689153, 1.00599428, 0.99477256, 1.00042081,\n",
       "       0.98537947, 0.95606813, 0.93590023, 1.0013312 , 0.9843066 ,\n",
       "       0.9876895 , 1.00224611, 0.95142683, 0.94645215, 0.98598109,\n",
       "       0.97765001, 0.98611643, 1.00556612, 0.99338112, 0.97766909,\n",
       "       0.98011943, 0.97572914, 0.93281624, 0.93422691, 0.95738914,\n",
       "       0.98356669, 0.98721077, 0.97946307, 1.02869264, 1.01371415,\n",
       "       0.99272321, 1.00234754, 1.006663  , 0.93070254, 0.96868097,\n",
       "       0.93818827, 0.98244493, 0.97667408, 1.00618532, 0.98278392,\n",
       "       0.9407957 , 0.98283791, 0.9839046 , 0.982043  , 0.97906597,\n",
       "       0.99901616, 0.95916496, 0.98353485, 0.97635154, 0.99081049,\n",
       "       0.96595073, 0.93889294, 0.97886836, 1.03864152, 1.02113658,\n",
       "       1.01176758, 0.97577937, 0.98833381, 1.04743174, 0.98013691,\n",
       "       0.97252425, 0.97812424, 0.93623977, 0.94504767, 0.95536353,\n",
       "       0.96424741, 0.98528515, 0.97291888, 0.9592066 , 0.96092239,\n",
       "       0.97790793, 0.95281317, 0.97766995, 0.98762555, 0.97180765,\n",
       "       1.01294273, 0.99461537, 0.98218008, 0.94896919, 0.97186332,\n",
       "       1.01238495, 1.00518765, 1.03424553, 0.99889645, 0.94215876,\n",
       "       1.01011781, 0.99405967, 1.02004597, 0.99899672, 0.9633603 ,\n",
       "       0.954024  , 0.97220795, 0.93705269, 0.94526965, 0.96801205,\n",
       "       0.97173719, 0.95409685, 0.98146937, 0.95968175, 0.97084327,\n",
       "       0.94696202, 0.98180184, 0.973855  , 0.98702279, 0.9970627 ,\n",
       "       0.9767496 , 0.99582245, 1.01552761, 0.99746969, 1.03159354,\n",
       "       1.00500162, 0.95708312, 0.99842071, 1.0080449 , 1.00312158,\n",
       "       1.03530224, 1.02355604, 1.05652647, 1.05146276, 0.99051299,\n",
       "       0.96695862, 0.98783364, 0.91421426, 0.97513735, 0.93629485,\n",
       "       0.95656795, 0.99652457, 0.93075754, 0.98498033, 0.97044512,\n",
       "       0.98660579, 0.93493682, 0.94932423, 0.87568381, 0.95494352,\n",
       "       1.00395425, 0.94858019, 1.00277427, 1.0418624 , 1.02629175,\n",
       "       1.02679017, 1.02240643, 0.97733379, 0.99173185, 0.96012718,\n",
       "       0.93807645, 0.94578982, 0.9063526 , 0.91884891, 0.95621687,\n",
       "       0.99182452, 1.03510121, 0.95260835, 0.99866428, 0.94566294,\n",
       "       0.94855061, 0.90628943, 0.95871944, 0.93182785, 0.91988378,\n",
       "       0.89540986, 0.94391532, 0.99332942, 1.02268881, 1.02773339,\n",
       "       0.94970147, 0.98522674, 1.00737155, 0.99013093, 1.0074131 ,\n",
       "       1.02340354, 1.03368611, 0.97211931, 0.97997615, 0.96417045,\n",
       "       0.897818  , 0.90528524, 0.95482055, 0.91072524, 0.94158248,\n",
       "       0.98568764, 0.92839163, 0.98567426, 0.9366218 , 0.9387097 ,\n",
       "       0.93448354, 0.94772441, 0.93984281, 0.97050212, 1.00780617,\n",
       "       0.97213146, 1.0283916 , 0.98543333, 1.06791003, 1.00193455,\n",
       "       0.96377212, 0.99479623, 0.99814963, 1.0565487 , 0.97686186,\n",
       "       1.02682845, 1.03891666, 1.01146769, 1.01079971, 0.98258361,\n",
       "       1.02776271, 0.97380727, 0.95776151, 0.94486782, 1.0346531 ,\n",
       "       0.98165587, 0.96975301, 0.99485435, 0.97942403, 0.98902327,\n",
       "       1.04606632, 1.00449827, 0.9769971 , 1.04893298, 0.98041761,\n",
       "       1.0012713 , 1.05512876, 0.99415098, 1.01763168])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(all_Z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.exponential(size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_weights = weights / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ml_mean = [np.dot(normalized_weights, all_Z[:, i]) for i in range(all_Z.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(all_Z, axis=0), weighted_ml_mean, 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 784)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(all_Z[0, :] - weighted_ml_mean, (all_Z[0, :] - weighted_ml_mean)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ml_cov = np.zeros((all_Z.shape[1], all_Z.shape[1]))\n",
    "for i in range(all_Z.shape[0]):\n",
    "    weighted_ml_cov += normalized_weights[i] * np.outer(all_Z[i, :] - weighted_ml_mean, (all_Z[i, :] - weighted_ml_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(weighted_ml_cov)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_row = 10\n",
    "u = model.base_dist.sample((n_row**2, 1)).squeeze()\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, _ = model.inverse(u)\n",
    "log_probs = model.log_prob(samples).sort(0)[1].flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[log_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = samples.view(samples.shape[0], *args.input_dims)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = (torch.sigmoid(samples) - train_dataloader.dataset.lam) / (1 - 2 * train_dataloader.dataset.lam)\n",
    "save_image(samples, 'basic_samples.png', nrow=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = np.random.multivariate_normal(mean=weighted_ml_mean, cov=weighted_ml_cov, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = torch.from_numpy(u2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2, _ = model.inverse(u2.float())\n",
    "log_probs = model.log_prob(samples2).sort(0)[1].flip(0)\n",
    "samples2 = samples2[log_probs]\n",
    "samples2 = samples2.view(samples2.shape[0], *args.input_dims)\n",
    "samples2 = (torch.sigmoid(samples2) - train_dataloader.dataset.lam) / (1 - 2 * train_dataloader.dataset.lam)\n",
    "save_image(samples2, 'wlb_samples.png', nrow=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1705.3569, -1704.7877, -1570.6821, -1491.4120, -1455.0160, -1805.1970,\n",
       "        -1625.3429, -1642.6885, -1618.1909, -1685.9089, -1356.2938, -1669.7491,\n",
       "        -1603.5383, -1556.9873, -1550.9833, -1475.4789, -1629.1084, -1616.4868,\n",
       "        -1564.8269, -1562.3669, -1553.0282, -1601.2928, -1485.4480, -1666.8209,\n",
       "        -1533.0057, -1648.0332, -1746.2894, -1572.7708, -1641.4694, -1542.1995,\n",
       "        -1552.6294, -1400.4916, -1781.2429, -1560.3727, -1661.6193, -1650.3163,\n",
       "        -1658.0621, -1674.6748, -1554.9468, -1605.6161, -1401.5372, -1545.7150,\n",
       "        -1635.0905, -1567.3464, -1548.8174, -1648.7311, -1602.8871, -1570.4633,\n",
       "        -1657.1458, -1480.4958, -1609.6390, -1734.4767, -1612.4224, -1419.8280,\n",
       "        -1674.7410, -1397.0052, -1551.0342, -1568.6194, -1481.4043, -1617.1187,\n",
       "        -1584.9845, -1525.0138, -1566.6632, -1564.9041, -1373.5930, -1607.3364,\n",
       "        -1549.4485, -1614.0040, -1589.5908, -1683.3782, -1443.3512, -1615.9037,\n",
       "        -1538.1742, -1659.5015, -1633.8915, -1560.9299, -1526.6479, -1653.1328,\n",
       "        -1427.3920, -1593.7798, -1573.4565, -1588.9038, -1522.4266, -1601.9724,\n",
       "        -1656.2000, -1641.5880, -1671.3738, -1583.2715, -1654.9124, -1709.0521,\n",
       "        -1696.7317, -1552.8739, -1606.0857, -1566.5823, -1630.8538, -1642.8860,\n",
       "        -1587.4474, -1624.8232, -1668.6010, -1587.7158],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples2, _ = model.inverse(u2.float())\n",
    "model.log_prob(samples2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_ml_cov = np.zeros((all_Z.shape[1], all_Z.shape[1]))\n",
    "for i in range(all_Z.shape[0]):\n",
    "    unweighted_ml_cov +=np.outer(all_Z[i, :], (all_Z[i, :]))\n",
    "    \n",
    "unweighted_ml_cov = unweighted_ml_cov / all_Z.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "u3 = np.random.multivariate_normal(mean=np.zeros(all_Z.shape[1]), cov=unweighted_ml_cov, size=100)\n",
    "u3 = torch.from_numpy(u3)\n",
    "samples3, _ = model.inverse(u3.float())\n",
    "log_probs = model.log_prob(samples3).sort(0)[1].flip(0)\n",
    "samples3 = samples3[log_probs]\n",
    "samples3 = samples3.view(samples3.shape[0], *args.input_dims)\n",
    "samples3 = (torch.sigmoid(samples3) - train_dataloader.dataset.lam) / (1 - 2 * train_dataloader.dataset.lam)\n",
    "save_image(samples3, 'raw_weighted_samples.png', nrow=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(activation_fn='relu', batch_size=100, cond_label_size=None, conditional=False, data_dir='./data/', dataset='MNIST', device=device(type='cpu'), evaluate=False, flip_toy_var_order=False, generate=False, hidden_size=100, input_dims=(1, 28, 28), input_order='sequential', input_size=784, log_interval=1000, lr=0.0001, model='maf', n_blocks=5, n_components=1, n_epochs=50, n_hidden=1, no_batch_norm=False, no_cuda=False, output_dir='./results/maf_mnist', restore_file=None, results_file='./results/maf_mnist/results.txt', seed=1, start_epoch=0, train=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wlb_basic(model, dataset_lam, args, step=None, n_row=10):\n",
    "    model.eval()\n",
    "\n",
    "    u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "        samples, _ = model.inverse(u)\n",
    "        log_probs = model.log_prob(samples).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
    "        samples = samples[log_probs]\n",
    "\n",
    "    # convert and save images\n",
    "    samples = samples.view(samples.shape[0], *args.input_dims)\n",
    "    samples = (torch.sigmoid(samples) - dataset_lam) / (1 - 2 * dataset_lam)\n",
    "    filename = 'generated_samples' + (step != None)*'_epoch_{}'.format(step) + '.png'\n",
    "    save_image(samples, os.path.join(args.output_dir, filename), nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
