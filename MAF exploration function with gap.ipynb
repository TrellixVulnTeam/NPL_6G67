{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sineadwilliamson/Research/NF/normalizing_flows/data.py:122: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  kwargs = {'num_workers': 1, 'pin_memory': True} if device.type is 'cuda' else {}\n"
     ]
    }
   ],
   "source": [
    "from maf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--test_labels'], dest='test_labels', nargs=0, const=True, default=False, type=None, choices=None, help='Whether to use pre-specified Y labels for test generation.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
    "parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "parser.add_argument('--generate', action='store_true', help='Generate samples from a model.')\n",
    "parser.add_argument('--data_dir', default='./data/', help='Location of datasets.')\n",
    "parser.add_argument('--output_dir', default='./results/maf')\n",
    "parser.add_argument('--results_file', default='results.txt', help='Filename where to store settings and test results.')\n",
    "parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
    "# data\n",
    "parser.add_argument('--dataset', default='toy', help='Which dataset to use.')\n",
    "parser.add_argument('--flip_toy_var_order', action='store_true', help='Whether to flip the toy dataset variable order to (x2, x1).')\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed to use.')\n",
    "# model\n",
    "parser.add_argument('--model', default='maf', help='Which model to use: made, maf.')\n",
    "# made parameters\n",
    "parser.add_argument('--n_blocks', type=int, default=5, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
    "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
    "parser.add_argument('--hidden_size', type=int, default=100, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
    "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
    "parser.add_argument('--activation_fn', type=str, default='relu', help='What activation function to use in the MADEs.')\n",
    "parser.add_argument('--input_order', type=str, default='sequential', help='What input order to use (sequential | random).')\n",
    "parser.add_argument('--conditional', default=False, action='store_true', help='Whether to use a conditional model.')\n",
    "parser.add_argument('--no_batch_norm', action='store_true')\n",
    "# training params\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--n_epochs', type=int, default=50)\n",
    "parser.add_argument('--start_epoch', default=0, help='Starting epoch (for logging; to be overwritten when restoring file.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate.')\n",
    "parser.add_argument('--log_interval', type=int, default=1000, help='How often to show loss statistics and save samples.')\n",
    "parser.add_argument('--test_labels', default=False, action='store_true', help='Whether to use pre-specified Y labels for test generation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--train --model=maf --dataset=SYNTH --n_epochs=50 --conditional --batch_size=100 --test_labels\".split())\n",
    "#args = parser.parse_args(\"--train --model=maf --dataset=MOONS --n_epochs=50 --conditional\".split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cpu')\n",
    "torch.manual_seed(args.seed)\n",
    "if args.conditional: assert args.dataset in ['MNIST', 'CIFAR10', 'SYNTH', 'MOONS'], 'Conditional inputs only available for labeled datasets MNIST and CIFAR10.'\n",
    "train_dataloader, test_dataloader = fetch_dataloaders(args.dataset, args.batch_size, args.device, args.flip_toy_var_order)\n",
    "args.input_size = train_dataloader.dataset.input_size\n",
    "args.input_dims = train_dataloader.dataset.input_dims\n",
    "args.cond_label_size = train_dataloader.dataset.label_size if args.conditional else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAF(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                    args.activation_fn, args.input_order, batch_norm=not args.no_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded settings and model:\n",
      "{'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'cond_label_size': 1,\n",
      " 'conditional': True,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'SYNTH',\n",
      " 'device': device(type='cpu'),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/maf/results.txt',\n",
      " 'seed': 1,\n",
      " 'start_epoch': 0,\n",
      " 'test_labels': True,\n",
      " 'train': True}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True, cond_features=1)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BatchNorm()\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True, cond_features=1)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BatchNorm()\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True, cond_features=1)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BatchNorm()\n",
      "    (6): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True, cond_features=1)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BatchNorm()\n",
      "    (8): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True, cond_features=1)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): BatchNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args.results_file = os.path.join(args.output_dir, args.results_file)\n",
    "\n",
    "print('Loaded settings and model:')\n",
    "print(pprint.pformat(args.__dict__))\n",
    "print(model)\n",
    "print(pprint.pformat(args.__dict__), file=open(args.results_file, 'a'))\n",
    "print(model, file=open(args.results_file, 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 / 50, step    0 / 100; loss 1.1447\n",
      "Evaluate (epoch 0) -- logp(x) = -1.051 +/- 0.049\n",
      "epoch   1 / 50, step    0 / 100; loss 1.0360\n",
      "Evaluate (epoch 1) -- logp(x) = -0.851 +/- 0.046\n",
      "epoch   2 / 50, step    0 / 100; loss 0.8087\n",
      "Evaluate (epoch 2) -- logp(x) = -0.757 +/- 0.043\n",
      "epoch   3 / 50, step    0 / 100; loss 0.7655\n",
      "Evaluate (epoch 3) -- logp(x) = -0.710 +/- 0.039\n",
      "epoch   4 / 50, step    0 / 100; loss 0.7389\n",
      "Evaluate (epoch 4) -- logp(x) = -0.674 +/- 0.037\n",
      "epoch   5 / 50, step    0 / 100; loss 0.6927\n",
      "Evaluate (epoch 5) -- logp(x) = -0.626 +/- 0.043\n",
      "epoch   6 / 50, step    0 / 100; loss 0.5840\n",
      "Evaluate (epoch 6) -- logp(x) = -0.584 +/- 0.040\n",
      "epoch   7 / 50, step    0 / 100; loss 0.5587\n",
      "Evaluate (epoch 7) -- logp(x) = -0.548 +/- 0.039\n",
      "epoch   8 / 50, step    0 / 100; loss 0.6135\n",
      "Evaluate (epoch 8) -- logp(x) = -0.530 +/- 0.042\n",
      "epoch   9 / 50, step    0 / 100; loss 0.5485\n",
      "Evaluate (epoch 9) -- logp(x) = -0.489 +/- 0.044\n",
      "epoch  10 / 50, step    0 / 100; loss 0.5191\n",
      "Evaluate (epoch 10) -- logp(x) = -0.470 +/- 0.042\n",
      "epoch  11 / 50, step    0 / 100; loss 0.4634\n",
      "Evaluate (epoch 11) -- logp(x) = -0.419 +/- 0.046\n",
      "epoch  12 / 50, step    0 / 100; loss 0.3598\n",
      "Evaluate (epoch 12) -- logp(x) = -0.415 +/- 0.048\n",
      "epoch  13 / 50, step    0 / 100; loss 0.3684\n",
      "Evaluate (epoch 13) -- logp(x) = -0.307 +/- 0.048\n",
      "epoch  14 / 50, step    0 / 100; loss 0.3206\n",
      "Evaluate (epoch 14) -- logp(x) = -0.218 +/- 0.051\n",
      "epoch  15 / 50, step    0 / 100; loss 0.2126\n",
      "Evaluate (epoch 15) -- logp(x) = -0.100 +/- 0.049\n",
      "epoch  16 / 50, step    0 / 100; loss 0.0420\n",
      "Evaluate (epoch 16) -- logp(x) = 0.076 +/- 0.053\n",
      "epoch  17 / 50, step    0 / 100; loss 0.0050\n",
      "Evaluate (epoch 17) -- logp(x) = 0.285 +/- 0.063\n",
      "epoch  18 / 50, step    0 / 100; loss -0.3819\n",
      "Evaluate (epoch 18) -- logp(x) = -2.554 +/- 1.166\n",
      "epoch  19 / 50, step    0 / 100; loss -0.2618\n",
      "Evaluate (epoch 19) -- logp(x) = 0.480 +/- 0.064\n",
      "epoch  20 / 50, step    0 / 100; loss -0.5623\n",
      "Evaluate (epoch 20) -- logp(x) = 0.537 +/- 0.073\n",
      "epoch  21 / 50, step    0 / 100; loss -0.5066\n",
      "Evaluate (epoch 21) -- logp(x) = -0.211 +/- 0.190\n",
      "epoch  22 / 50, step    0 / 100; loss -0.7720\n",
      "Evaluate (epoch 22) -- logp(x) = 0.227 +/- 0.126\n",
      "epoch  23 / 50, step    0 / 100; loss -0.7544\n",
      "Evaluate (epoch 23) -- logp(x) = 0.855 +/- 0.069\n",
      "epoch  24 / 50, step    0 / 100; loss -0.9978\n",
      "Evaluate (epoch 24) -- logp(x) = 0.851 +/- 0.078\n",
      "epoch  25 / 50, step    0 / 100; loss -0.8808\n",
      "Evaluate (epoch 25) -- logp(x) = 0.611 +/- 0.103\n",
      "epoch  26 / 50, step    0 / 100; loss -1.0698\n",
      "Evaluate (epoch 26) -- logp(x) = -0.045 +/- 0.212\n",
      "epoch  27 / 50, step    0 / 100; loss -0.6536\n",
      "Evaluate (epoch 27) -- logp(x) = -0.010 +/- 0.198\n",
      "epoch  28 / 50, step    0 / 100; loss -0.9945\n",
      "Evaluate (epoch 28) -- logp(x) = 0.777 +/- 0.086\n",
      "epoch  29 / 50, step    0 / 100; loss -1.0954\n",
      "Evaluate (epoch 29) -- logp(x) = 1.020 +/- 0.069\n",
      "epoch  30 / 50, step    0 / 100; loss -1.2054\n",
      "Evaluate (epoch 30) -- logp(x) = 1.003 +/- 0.082\n",
      "epoch  31 / 50, step    0 / 100; loss -1.2072\n",
      "Evaluate (epoch 31) -- logp(x) = 1.179 +/- 0.069\n",
      "epoch  32 / 50, step    0 / 100; loss -1.1752\n",
      "Evaluate (epoch 32) -- logp(x) = 1.179 +/- 0.077\n",
      "epoch  33 / 50, step    0 / 100; loss -1.3623\n",
      "Evaluate (epoch 33) -- logp(x) = 1.252 +/- 0.065\n",
      "epoch  34 / 50, step    0 / 100; loss -1.3913\n",
      "Evaluate (epoch 34) -- logp(x) = 1.308 +/- 0.069\n",
      "epoch  35 / 50, step    0 / 100; loss -1.2803\n",
      "Evaluate (epoch 35) -- logp(x) = 1.205 +/- 0.070\n",
      "epoch  36 / 50, step    0 / 100; loss -1.1587\n",
      "Evaluate (epoch 36) -- logp(x) = 1.225 +/- 0.063\n",
      "epoch  37 / 50, step    0 / 100; loss -1.2061\n",
      "Evaluate (epoch 37) -- logp(x) = 1.321 +/- 0.070\n",
      "epoch  38 / 50, step    0 / 100; loss -1.4713\n",
      "Evaluate (epoch 38) -- logp(x) = 1.257 +/- 0.089\n",
      "epoch  39 / 50, step    0 / 100; loss -1.3714\n",
      "Evaluate (epoch 39) -- logp(x) = 1.273 +/- 0.075\n",
      "epoch  40 / 50, step    0 / 100; loss -1.4399\n",
      "Evaluate (epoch 40) -- logp(x) = 1.399 +/- 0.071\n",
      "epoch  41 / 50, step    0 / 100; loss -1.4489\n",
      "Evaluate (epoch 41) -- logp(x) = 0.829 +/- 0.140\n",
      "epoch  42 / 50, step    0 / 100; loss -1.2366\n",
      "Evaluate (epoch 42) -- logp(x) = 1.255 +/- 0.074\n",
      "epoch  43 / 50, step    0 / 100; loss -1.4268\n",
      "Evaluate (epoch 43) -- logp(x) = 1.313 +/- 0.113\n",
      "epoch  44 / 50, step    0 / 100; loss -1.5040\n",
      "Evaluate (epoch 44) -- logp(x) = 1.237 +/- 0.208\n",
      "epoch  45 / 50, step    0 / 100; loss -1.4584\n",
      "Evaluate (epoch 45) -- logp(x) = 1.272 +/- 0.079\n",
      "epoch  46 / 50, step    0 / 100; loss -1.4377\n",
      "Evaluate (epoch 46) -- logp(x) = 1.408 +/- 0.067\n",
      "epoch  47 / 50, step    0 / 100; loss -1.2318\n",
      "Evaluate (epoch 47) -- logp(x) = 1.370 +/- 0.075\n",
      "epoch  48 / 50, step    0 / 100; loss -1.2187\n",
      "Evaluate (epoch 48) -- logp(x) = 1.306 +/- 0.212\n",
      "epoch  49 / 50, step    0 / 100; loss -1.5653\n",
      "Evaluate (epoch 49) -- logp(x) = 1.263 +/- 0.082\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty(model, args, input_range=[0., 1.], train_X=None, train_Y=None, plot_dim=1):\n",
    "    Xvals = np.linspace(input_range[0], input_range[1], 100)\n",
    "    X = torch.Tensor(Xvals)\n",
    "    X = X.view(X.shape[0], -1)\n",
    "    \n",
    "    Ys = np.zeros((100, X.shape[0]))\n",
    "    model.eval()\n",
    "    for i in range(100):\n",
    "        u = model.base_dist.sample((X.shape[0], args.n_components)).squeeze()\n",
    "        Y, _ = model.inverse(u, X)\n",
    "        Ys[i, :] = Y.detach().numpy()[:, plot_dim]\n",
    "    \n",
    "    Ymean = np.mean(Ys, axis=0)\n",
    "    Ysd = np.std(Ys, axis=0)\n",
    "    plt.plot(Xvals, Ymean, 'k')\n",
    "    plt.plot(Xvals, Ymean+Ysd, 'b')\n",
    "    plt.plot(Xvals, Ymean-Ysd, 'b')\n",
    "    if train_X is not None:\n",
    "        if train_Y is not None:\n",
    "            plt.plot(train_X, train_Y[:, plot_dim], 'rx')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = next(iter(train_dataloader))\n",
    "Y = data[0].detach().numpy()\n",
    "X = data[1].detach().numpy()\n",
    "plot_uncertainty(model, args, train_X=X, train_Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blah' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5e4ab9400f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblah\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'blah' is not defined"
     ]
    }
   ],
   "source": [
    "print(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "n_row=10\n",
    "#u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "#samples, _ = model.inverse(u)\n",
    "\n",
    "u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "rolls = make_swiss_roll(n_row**2, noise=0.05)\n",
    "Xdist = D.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "X = Xdist.sample(torch.Size([n_row**2]))\n",
    "X = torch.Tensor(X)\n",
    "X = X.view(X.shape[0], -1)\n",
    "\n",
    "Y, _ = model.inverse(u, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "Xn = X.numpy()\n",
    "Yn = Y.detach().numpy()\n",
    "ax.scatter(Xn, Yn[:, 0], Yn[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.plot(Xn, 2*Xn - 1. + np.sin(15*Xn), 'x')\n",
    "plt.plot(Xn, Yn[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Yn[:, 0], Yn[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.plot(data[1].detach().numpy(), data[0].detach().numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[0].detach().numpy()[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgen, Ygen = generate(model, train_dataloader.dataset.lam, args, Xmin=0.0, Xmax=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xgen.numpy(), Ygen.numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].numpy()[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[0].numpy()[:, 0], data[0].numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "n_row=10\n",
    "#u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "#samples, _ = model.inverse(u)\n",
    "\n",
    "u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "rolls = make_swiss_roll(n_row**2, noise=0.05)\n",
    "X = torch.Tensor(rolls[0][:, 0])\n",
    "X = X.view(X.shape[0], -1)\n",
    "\n",
    "Y, _ = model.inverse(u, X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "Yn = Y.detach().numpy()\n",
    "Xn = X.numpy()\n",
    "ax.scatter(Xn, Yn[:, 0], Yn[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples.detach().numpy()[:, 0], samples.detach().numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "X = samples.detach().numpy()\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls = make_swiss_roll(1000, noise=0.05)\n",
    "plt.plot(rolls[0][:, 0], rolls[0][:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
