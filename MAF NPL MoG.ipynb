{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sineadwilliamson/Research/NF/normalizing_flows/data.py:129: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  kwargs = {'num_workers': 1, 'pin_memory': True} if device.type is 'cuda' else {}\n"
     ]
    }
   ],
   "source": [
    "from maf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--T'], dest='T', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help='number of prior samples', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
    "parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "parser.add_argument('--generate', action='store_true', help='Generate samples from a model.')\n",
    "parser.add_argument('--data_dir', default='./data/', help='Location of datasets.')\n",
    "parser.add_argument('--output_dir', default='./results/maf')\n",
    "parser.add_argument('--results_file', default='results.txt', help='Filename where to store settings and test results.')\n",
    "parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
    "# data\n",
    "parser.add_argument('--dataset', default='toy', help='Which dataset to use.')\n",
    "parser.add_argument('--flip_toy_var_order', action='store_true', help='Whether to flip the toy dataset variable order to (x2, x1).')\n",
    "parser.add_argument('--seed', type=int, default=1, help='Random seed to use.')\n",
    "# model\n",
    "parser.add_argument('--model', default='maf', help='Which model to use: made, maf.')\n",
    "# made parameters\n",
    "parser.add_argument('--n_blocks', type=int, default=5, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
    "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
    "parser.add_argument('--hidden_size', type=int, default=100, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
    "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
    "parser.add_argument('--activation_fn', type=str, default='relu', help='What activation function to use in the MADEs.')\n",
    "parser.add_argument('--input_order', type=str, default='sequential', help='What input order to use (sequential | random).')\n",
    "parser.add_argument('--conditional', default=False, action='store_true', help='Whether to use a conditional model.')\n",
    "parser.add_argument('--no_batch_norm', action='store_true')\n",
    "# training params\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--n_epochs', type=int, default=50)\n",
    "parser.add_argument('--start_epoch', default=0, help='Starting epoch (for logging; to be overwritten when restoring file.')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate.')\n",
    "parser.add_argument('--log_interval', type=int, default=1000, help='How often to show loss statistics and save samples.')\n",
    "parser.add_argument('--test_labels', default=False, action='store_true', help='Whether to use pre-specified Y labels for test generation.')\n",
    "parser.add_argument('--use_weights', default=False, action='store_true', help='Whether to use weights.')\n",
    "parser.add_argument('--concentration', type=float, default=0.0, help='NPL concentration parameter')\n",
    "parser.add_argument('--T', type=int, default=10, help='number of prior samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parser.parse_args(\"--train --model=maf --dataset=GAUSSIANMIX --n_epochs=50 --batch_size=100 --use_weights --concentration=0.0 --T=10 --seed=4\".split())\n",
    "args = parser.parse_args(\"--train --model=maf --dataset=GAUSSIANMIX --n_epochs=50 --batch_size=100 --concentration=0.0 --T=10 --seed=4\".split())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cpu')\n",
    "torch.manual_seed(args.seed)\n",
    "if args.conditional: assert args.dataset in ['MNIST', 'CIFAR10', 'SYNTH', 'MOONS'], 'Conditional inputs only available for labeled datasets MNIST and CIFAR10.'\n",
    "train_dataloader, test_dataloader = fetch_dataloaders(args.dataset, args.batch_size, args.device, args.flip_toy_var_order)\n",
    "args.input_size = train_dataloader.dataset.input_size\n",
    "args.input_dims = train_dataloader.dataset.input_dims\n",
    "args.cond_label_size = train_dataloader.dataset.label_size if args.conditional else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAF(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
    "                    args.activation_fn, args.input_order, batch_norm=not args.no_batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "train_data = train_dataloader.dataset.X\n",
    "\n",
    "train_data = train_data.detach().numpy()\n",
    "#plt.scatter(train_data[:, 0], train_data[:, 1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded settings and model:\n",
      "{'T': 10,\n",
      " 'activation_fn': 'relu',\n",
      " 'batch_size': 100,\n",
      " 'concentration': 0.0,\n",
      " 'cond_label_size': None,\n",
      " 'conditional': False,\n",
      " 'data_dir': './data/',\n",
      " 'dataset': 'GAUSSIANMIX',\n",
      " 'device': device(type='cpu'),\n",
      " 'evaluate': False,\n",
      " 'flip_toy_var_order': False,\n",
      " 'generate': False,\n",
      " 'hidden_size': 100,\n",
      " 'input_dims': 2,\n",
      " 'input_order': 'sequential',\n",
      " 'input_size': 2,\n",
      " 'log_interval': 1000,\n",
      " 'lr': 0.0001,\n",
      " 'model': 'maf',\n",
      " 'n_blocks': 5,\n",
      " 'n_components': 1,\n",
      " 'n_epochs': 50,\n",
      " 'n_hidden': 1,\n",
      " 'no_batch_norm': False,\n",
      " 'no_cuda': False,\n",
      " 'output_dir': './results/maf',\n",
      " 'restore_file': None,\n",
      " 'results_file': './results/maf/results.txt',\n",
      " 'seed': 4,\n",
      " 'start_epoch': 0,\n",
      " 'test_labels': False,\n",
      " 'train': True,\n",
      " 'use_weights': False}\n",
      "MAF(\n",
      "  (net): FlowSequential(\n",
      "    (0): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BatchNorm()\n",
      "    (2): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BatchNorm()\n",
      "    (4): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): BatchNorm()\n",
      "    (6): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): BatchNorm()\n",
      "    (8): MADE(\n",
      "      (net_input): MaskedLinear(in_features=2, out_features=100, bias=True)\n",
      "      (net): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): MaskedLinear(in_features=100, out_features=100, bias=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaskedLinear(in_features=100, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): BatchNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args.results_file = os.path.join(args.output_dir, args.results_file)\n",
    "\n",
    "print('Loaded settings and model:')\n",
    "print(pprint.pformat(args.__dict__))\n",
    "print(model)\n",
    "print(pprint.pformat(args.__dict__), file=open(args.results_file, 'a'))\n",
    "print(model, file=open(args.results_file, 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 / 50, step    0 / 10; loss 2.7395\n",
      "Evaluate (epoch 0) -- logp(x) = -2.764 +/- 0.031\n",
      "epoch   1 / 50, step    0 / 10; loss 2.6537\n",
      "Evaluate (epoch 1) -- logp(x) = -2.559 +/- 0.037\n",
      "epoch   2 / 50, step    0 / 10; loss 2.5980\n",
      "Evaluate (epoch 2) -- logp(x) = -2.472 +/- 0.049\n",
      "epoch   3 / 50, step    0 / 10; loss 2.4012\n",
      "Evaluate (epoch 3) -- logp(x) = -2.430 +/- 0.055\n",
      "epoch   4 / 50, step    0 / 10; loss 2.4918\n",
      "Evaluate (epoch 4) -- logp(x) = -2.395 +/- 0.056\n",
      "epoch   5 / 50, step    0 / 10; loss 2.4893\n",
      "Evaluate (epoch 5) -- logp(x) = -2.362 +/- 0.057\n",
      "epoch   6 / 50, step    0 / 10; loss 2.3981\n",
      "Evaluate (epoch 6) -- logp(x) = -2.334 +/- 0.062\n",
      "epoch   7 / 50, step    0 / 10; loss 2.2737\n",
      "Evaluate (epoch 7) -- logp(x) = -2.307 +/- 0.064\n",
      "epoch   8 / 50, step    0 / 10; loss 2.2513\n",
      "Evaluate (epoch 8) -- logp(x) = -2.287 +/- 0.069\n",
      "epoch   9 / 50, step    0 / 10; loss 2.1793\n",
      "Evaluate (epoch 9) -- logp(x) = -2.270 +/- 0.073\n",
      "epoch  10 / 50, step    0 / 10; loss 2.2131\n",
      "Evaluate (epoch 10) -- logp(x) = -2.251 +/- 0.074\n",
      "epoch  11 / 50, step    0 / 10; loss 2.4252\n",
      "Evaluate (epoch 11) -- logp(x) = -2.232 +/- 0.077\n",
      "epoch  12 / 50, step    0 / 10; loss 2.2493\n",
      "Evaluate (epoch 12) -- logp(x) = -2.211 +/- 0.083\n",
      "epoch  13 / 50, step    0 / 10; loss 2.1318\n",
      "Evaluate (epoch 13) -- logp(x) = -2.186 +/- 0.085\n",
      "epoch  14 / 50, step    0 / 10; loss 2.3352\n",
      "Evaluate (epoch 14) -- logp(x) = -2.152 +/- 0.092\n",
      "epoch  15 / 50, step    0 / 10; loss 2.1753\n",
      "Evaluate (epoch 15) -- logp(x) = -2.118 +/- 0.096\n",
      "epoch  16 / 50, step    0 / 10; loss 2.1030\n",
      "Evaluate (epoch 16) -- logp(x) = -2.092 +/- 0.111\n",
      "epoch  17 / 50, step    0 / 10; loss 1.9692\n",
      "Evaluate (epoch 17) -- logp(x) = -2.064 +/- 0.121\n",
      "epoch  18 / 50, step    0 / 10; loss 2.0845\n",
      "Evaluate (epoch 18) -- logp(x) = -2.031 +/- 0.111\n",
      "epoch  19 / 50, step    0 / 10; loss 2.0850\n",
      "Evaluate (epoch 19) -- logp(x) = -2.039 +/- 0.121\n",
      "epoch  20 / 50, step    0 / 10; loss 1.9362\n",
      "Evaluate (epoch 20) -- logp(x) = -2.023 +/- 0.118\n",
      "epoch  21 / 50, step    0 / 10; loss 1.9856\n",
      "Evaluate (epoch 21) -- logp(x) = -2.014 +/- 0.122\n",
      "epoch  22 / 50, step    0 / 10; loss 1.7386\n",
      "Evaluate (epoch 22) -- logp(x) = -2.003 +/- 0.117\n",
      "epoch  23 / 50, step    0 / 10; loss 1.9595\n",
      "Evaluate (epoch 23) -- logp(x) = -2.003 +/- 0.129\n",
      "epoch  24 / 50, step    0 / 10; loss 2.0531\n",
      "Evaluate (epoch 24) -- logp(x) = -1.992 +/- 0.126\n",
      "epoch  25 / 50, step    0 / 10; loss 2.0554\n",
      "Evaluate (epoch 25) -- logp(x) = -1.986 +/- 0.132\n",
      "epoch  26 / 50, step    0 / 10; loss 1.9109\n",
      "Evaluate (epoch 26) -- logp(x) = -1.990 +/- 0.144\n",
      "epoch  27 / 50, step    0 / 10; loss 1.9020\n",
      "Evaluate (epoch 27) -- logp(x) = -1.979 +/- 0.136\n",
      "epoch  28 / 50, step    0 / 10; loss 2.2530\n",
      "Evaluate (epoch 28) -- logp(x) = -1.977 +/- 0.127\n",
      "epoch  29 / 50, step    0 / 10; loss 2.0498\n",
      "Evaluate (epoch 29) -- logp(x) = -1.964 +/- 0.122\n",
      "epoch  30 / 50, step    0 / 10; loss 1.9492\n",
      "Evaluate (epoch 30) -- logp(x) = -1.959 +/- 0.138\n",
      "epoch  31 / 50, step    0 / 10; loss 1.9913\n",
      "Evaluate (epoch 31) -- logp(x) = -1.956 +/- 0.138\n",
      "epoch  32 / 50, step    0 / 10; loss 2.0397\n",
      "Evaluate (epoch 32) -- logp(x) = -1.952 +/- 0.145\n",
      "epoch  33 / 50, step    0 / 10; loss 1.9530\n",
      "Evaluate (epoch 33) -- logp(x) = -1.942 +/- 0.135\n",
      "epoch  34 / 50, step    0 / 10; loss 1.9065\n",
      "Evaluate (epoch 34) -- logp(x) = -1.934 +/- 0.140\n",
      "epoch  35 / 50, step    0 / 10; loss 1.9920\n",
      "Evaluate (epoch 35) -- logp(x) = -1.926 +/- 0.138\n",
      "epoch  36 / 50, step    0 / 10; loss 1.8127\n",
      "Evaluate (epoch 36) -- logp(x) = -1.919 +/- 0.126\n",
      "epoch  37 / 50, step    0 / 10; loss 1.8935\n",
      "Evaluate (epoch 37) -- logp(x) = -1.914 +/- 0.126\n",
      "epoch  38 / 50, step    0 / 10; loss 1.8635\n",
      "Evaluate (epoch 38) -- logp(x) = -1.906 +/- 0.126\n",
      "epoch  39 / 50, step    0 / 10; loss 1.9124\n",
      "Evaluate (epoch 39) -- logp(x) = -1.899 +/- 0.120\n",
      "epoch  40 / 50, step    0 / 10; loss 1.8672\n",
      "Evaluate (epoch 40) -- logp(x) = -1.892 +/- 0.114\n",
      "epoch  41 / 50, step    0 / 10; loss 1.8345\n",
      "Evaluate (epoch 41) -- logp(x) = -1.877 +/- 0.108\n",
      "epoch  42 / 50, step    0 / 10; loss 1.8678\n",
      "Evaluate (epoch 42) -- logp(x) = -1.886 +/- 0.118\n",
      "epoch  43 / 50, step    0 / 10; loss 1.9509\n",
      "Evaluate (epoch 43) -- logp(x) = -1.862 +/- 0.099\n",
      "epoch  44 / 50, step    0 / 10; loss 1.9230\n",
      "Evaluate (epoch 44) -- logp(x) = -1.868 +/- 0.106\n",
      "epoch  45 / 50, step    0 / 10; loss 1.9942\n",
      "Evaluate (epoch 45) -- logp(x) = -1.849 +/- 0.098\n",
      "epoch  46 / 50, step    0 / 10; loss 1.9475\n",
      "Evaluate (epoch 46) -- logp(x) = -1.838 +/- 0.096\n",
      "epoch  47 / 50, step    0 / 10; loss 1.8000\n",
      "Evaluate (epoch 47) -- logp(x) = -1.836 +/- 0.093\n",
      "epoch  48 / 50, step    0 / 10; loss 1.8426\n",
      "Evaluate (epoch 48) -- logp(x) = -1.825 +/- 0.090\n",
      "epoch  49 / 50, step    0 / 10; loss 2.0743\n",
      "Evaluate (epoch 49) -- logp(x) = -1.806 +/- 0.089\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(1000):\n",
    "    u = model.base_dist.sample((1, args.n_components)).squeeze(0)\n",
    "    s, _ = model.inverse(u)\n",
    "    samples.append(s.detach().numpy())\n",
    "samples = np.array(samples).squeeze()\n",
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-6d12df61c0b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeancovs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NF/normalizing_flows/maf.py\u001b[0m in \u001b[0;36mlearn_dist\u001b[0;34m(model, dataloader, args, num_samples)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlearn_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcentration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mprior_weight_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcentration\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "meancovs = learn_dist(model, train_dataloader, args, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(1000):\n",
    "    prior_dist = D.MultivariateNormal(meancovs[i][0], meancovs[i][1])\n",
    "    u = model.base_dist.sample((1, args.n_components)).squeeze(0)\n",
    "    s, _ = model.inverse(u)\n",
    "    samples.append(s.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "samples = np.array(samples).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plt.scatter(samples2[:, 0], samples[:, 1])\n",
    "plt.scatter(base_samples[:, 0], base_samples[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty(model, args, input_range=[0., 1.], train_X=None, train_Y=None, plot_dim=1):\n",
    "    Xvals = np.linspace(input_range[0], input_range[1], 100)\n",
    "    X = torch.Tensor(Xvals)\n",
    "    X = X.view(X.shape[0], -1)\n",
    "    \n",
    "    Ys = np.zeros((100, X.shape[0]))\n",
    "    model.eval()\n",
    "    for i in range(100):\n",
    "        u = model.base_dist.sample((X.shape[0], args.n_components)).squeeze()\n",
    "        Y, _ = model.inverse(u, X)\n",
    "        Ys[i, :] = Y.detach().numpy()[:, plot_dim]\n",
    "    \n",
    "    Ymean = np.mean(Ys, axis=0)\n",
    "    Ysd = np.std(Ys, axis=0)\n",
    "    plt.plot(Xvals, Ymean, 'k')\n",
    "    plt.plot(Xvals, Ymean+Ysd, 'b')\n",
    "    plt.plot(Xvals, Ymean-Ysd, 'b')\n",
    "    if train_X is not None:\n",
    "        if train_Y is not None:\n",
    "            plt.plot(train_X, train_Y[:, plot_dim], 'rx')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = next(iter(train_dataloader))\n",
    "Y = data[0].detach().numpy()\n",
    "X = data[1].detach().numpy()\n",
    "plot_uncertainty(model, args, train_X=X, train_Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "n_row=10\n",
    "#u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "#samples, _ = model.inverse(u)\n",
    "\n",
    "u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "rolls = make_swiss_roll(n_row**2, noise=0.05)\n",
    "Xdist = D.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "X = Xdist.sample(torch.Size([n_row**2]))\n",
    "X = torch.Tensor(X)\n",
    "X = X.view(X.shape[0], -1)\n",
    "\n",
    "Y, _ = model.inverse(u, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "Xn = X.numpy()\n",
    "Yn = Y.detach().numpy()\n",
    "ax.scatter(Xn, Yn[:, 0], Yn[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.plot(Xn, 2*Xn - 1. + np.sin(15*Xn), 'x')\n",
    "plt.plot(Xn, Yn[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Yn[:, 0], Yn[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.plot(data[1].detach().numpy(), data[0].detach().numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[0].detach().numpy()[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgen, Ygen = generate(model, train_dataloader.dataset.lam, args, Xmin=0.0, Xmax=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xgen.numpy(), Ygen.numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].numpy()[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[0].numpy()[:, 0], data[0].numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "n_row=10\n",
    "#u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "#samples, _ = model.inverse(u)\n",
    "\n",
    "u = model.base_dist.sample((n_row**2, args.n_components)).squeeze()\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "rolls = make_swiss_roll(n_row**2, noise=0.05)\n",
    "X = torch.Tensor(rolls[0][:, 0])\n",
    "X = X.view(X.shape[0], -1)\n",
    "\n",
    "Y, _ = model.inverse(u, X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "Yn = Y.detach().numpy()\n",
    "Xn = X.numpy()\n",
    "ax.scatter(Xn, Yn[:, 0], Yn[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples.detach().numpy()[:, 0], samples.detach().numpy()[:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "X = samples.detach().numpy()\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls = make_swiss_roll(1000, noise=0.05)\n",
    "plt.plot(rolls[0][:, 0], rolls[0][:, 1], 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
